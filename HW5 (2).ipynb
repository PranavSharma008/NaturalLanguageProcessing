{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw5MCSe39R4D"
      },
      "source": [
        "In this homework you will use code in a Jupyter notebook to fetch articles from Wikipedia and strip out the tags using “Beautiful Soup” (see accompanying file data_fetch.py). Then you will use keras to create an RNN model which can generate an altogether new sentence based on the body of content associated articles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ExKA1GY0eJZ"
      },
      "source": [
        "\n",
        "##1.Run the code provided in the data_fetch.py and insert your own Wikipedia article URL to retrieve a list of documents representing Wikipedia articles linked from the one provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5pe9zO32bb5"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "\n",
        "\n",
        "# Place the URL to your wikipedia page here:\n",
        "##########################################################\n",
        "wiki_url = 'https://en.wikipedia.org/wiki/Thomas_Jefferson'\n",
        "##########################################################\n",
        "\n",
        "\n",
        "def extract_wikipedia_links(url):\n",
        "  \"\"\"\n",
        "  Returns a list of linked wikipedia urls from an initial wikipedia page\n",
        "  \"\"\"\n",
        "  data = urllib.request.urlopen(url)\n",
        "  article = data.read()\n",
        "  html_article = bs.BeautifulSoup(article,'html')\n",
        "\n",
        "  hrefs = [tag.get('href') for tag in html_article.find_all('a', {'href': re.compile('/wiki/[a-zA-Z]')})]\n",
        "  hrefs_pattern1 = re.compile('.*:.*')\n",
        "  hrefs_pattern2 = re.compile('.*\\/\\/')\n",
        "  wiki_hrefs = list(set(\n",
        "      ['https://en.wikipedia.org'+href for href in hrefs if not hrefs_pattern1.match(\n",
        "          href) and not hrefs_pattern2.match(href)]))\n",
        "  return wiki_hrefs\n",
        "\n",
        "\n",
        "\n",
        "def extract_wikipedia_text(url):\n",
        "  \"\"\"\n",
        "  Returns the text from the body of a wikepedia article\n",
        "  \"\"\"\n",
        "  data = urllib.request.urlopen(url)\n",
        "  article = data.read()\n",
        "  html_article = bs.BeautifulSoup(article,'html')\n",
        "  paragraphs = html_article.find_all('p')\n",
        "  paragraph_list = []\n",
        "  pattern_1 = re.compile('\\[[0-9]*\\]')\n",
        "  pattern_2 = re.compile('\\s+')\n",
        "  for p in paragraphs:\n",
        "    paragraph_text = p.text\n",
        "    clean_paragraph_text = re.sub(\n",
        "                              pattern_2, ' ', re.sub(\n",
        "                                  pattern_1, '', paragraph_text))\n",
        "    paragraph_list.append(clean_paragraph_text)\n",
        "  if len(paragraph_list) == 0:\n",
        "    return ''\n",
        "  return paragraph_list[0] + ' '.join(paragraph_list[1:])\n",
        "\n",
        "\n",
        "\n",
        "def get_wiki_content(url):\n",
        "  \"\"\"\n",
        "  Returns a list of texts from the body of all linked wikepedia articles\n",
        "  \"\"\"\n",
        "  wiki_links = extract_wikipedia_links(url)\n",
        "  wiki_text = []\n",
        "  \n",
        "  for link in wiki_links:\n",
        "    text = extract_wikipedia_text(link)\n",
        "    wiki_text.append(text)\n",
        "\n",
        "  return wiki_text\n",
        "\n",
        "wiki_text_list = get_wiki_content(wiki_url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuHgsBb111cq"
      },
      "source": [
        "paras= extract_wikipedia_text(wiki_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlhua-vFYkhh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geyrDDpL0wV6"
      },
      "source": [
        "\n",
        "##2.Load the pre-trained spaCy pipeline called “en_core_web_lg” and then import and load it. Note that the pipeline takes two or three minutes to download because of the large amount of word embedding data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRHc9oGp7C57",
        "outputId": "3605b3c5-48ad-443f-c1c1-e10d361c39f8"
      },
      "source": [
        "\n",
        "import spacy.cli # use the command line interface\n",
        "spacy.cli.download(\"en_core_web_lg\") # this import the large model onto your virtual machine\n",
        "import en_core_web_lg\n",
        "nlp_lg = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FeRlNDp0hdI"
      },
      "source": [
        "\n",
        "##3.Pass each sentence into separate NLP documents via the spaCy pipeline. Do not remove any words or punctuation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZHNBceEk_bR"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXmNQbpQ756I",
        "outputId": "a68be378-fafa-40f3-ee3a-1525b06f1bac"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgM83hZT-3US",
        "outputId": "65cbe8f5-cca8-43b6-9ada-4790770095b2"
      },
      "source": [
        "sentences = nltk.sent_tokenize(paras)\n",
        "array1 = []\n",
        "array2_meanVector = []\n",
        "array_test=[]\n",
        "len(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "699"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UplNtIq8qzyd"
      },
      "source": [
        "for z in nlp_lg(sentences[0]):\n",
        "    array1.append(nlp_lg.vocab[\"z\"].vector)\n",
        "meaVector = np.mean(array1, axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A5Yzk9ktPH5",
        "outputId": "086c41c6-65d6-4299-c724-e8f15df655db"
      },
      "source": [
        "str(nlp_lg(sentences[0])[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Thomas'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRLFW21MBHig"
      },
      "source": [
        "for k in range(0,len(sentences)):\n",
        "  for z in range(0,len(nlp_lg(sentences[k]))):\n",
        "    array1.append(nlp_lg.vocab[str(nlp_lg(sentences[k])[z])].vector)\n",
        "  meanVector = np.mean(array1, axis=0)\n",
        "  array1=[]\n",
        "  array2_meanVector.append(meanVector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMKmEm6BnA1L",
        "outputId": "332724ee-13e9-4a32-fa27-c8bf98236128"
      },
      "source": [
        "np.mean(meanVector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.005496397"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8JGGR0Z0lJ6"
      },
      "source": [
        "\n",
        "##4.Compute the mean word vector across all the word vectors in each document (refer to Homework 3).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvz-Lq1FngJ2",
        "outputId": "f6c02709-0c6d-47ca-8537-bb8d2fa898b0"
      },
      "source": [
        "# array2_meanVector is the mean vector across all the documents, calculated above\n",
        "len(array2_meanVector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "699"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFvKGNHX1ZEy"
      },
      "source": [
        "\n",
        "##5.Use cosine similarity to create similarity scores between each pair of mean word vectors. You should end up with floating point values bounded between 0 and 1 for every pair of vectors. Make sure to avoid comparing a word vector to itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaXpbXTEogsO"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "def cosine(variableA,variableB):\n",
        "    return np.dot(variableA,variableB) / (np.sqrt(np.dot(variableA,variableA)) * np.sqrt(np.dot(variableB,variableB)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF37S3h1scSZ",
        "outputId": "5d0f8a89-c8dd-472e-8f20-5963bee526d9"
      },
      "source": [
        "len(array2_meanVector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "699"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMQJ4M731bpU"
      },
      "source": [
        "\n",
        "##6.Interpret the results from the previous step. Determine the two Wikipedia articles with the lowest cosine similarity and the two articles with the greatest cosine similarity scores. Which two articles are the most similar to each other? Which two are least similar? Does the pattern of values make sense? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "151oO3TVCnOB"
      },
      "source": [
        "import pandas as pd\n",
        "cosine_values_arry = []\n",
        "freeza=[[]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpyVLBgd33aX"
      },
      "source": [
        "for k in range(0,len(array2_meanVector)):\n",
        "  for z in range(0,len(array2_meanVector)):\n",
        "    if(k!=z):\n",
        "      cosine_values_arry.append(cosine(array2_meanVector[k],array2_meanVector[z]))\n",
        "  freeza.append(cosine_values_arry)\n",
        "  #print(cosine_values_arry)\n",
        "  cosine_values_arry = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n90-C2lU33fj"
      },
      "source": [
        "df= pd.DataFrame(freeza)\n",
        "min_arry =[]\n",
        "max_arry =[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYVSaaFSLQ_1"
      },
      "source": [
        "for i in range(0,692):\n",
        "  min_arry.append(df[i].min(skipna=True))\n",
        "  max_arry.append(df[i].max(skipna=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kro-9HaFVEKy",
        "outputId": "920cbe27-77f9-45b0-fc4a-8ebd9308bda1"
      },
      "source": [
        "min(min_arry)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2584898769855499"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD5c3glGVH77",
        "outputId": "4bc33c40-24f8-46e1-aba0-03e829d74bf0"
      },
      "source": [
        "max(max_arry)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.994910717010498"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuVhO4Hm1eHl"
      },
      "source": [
        "\n",
        "##7.Create ragged tensors of individual characters as well as of index IDs (see lab 9) for all .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF_vgUIOI65C",
        "outputId": "7796d031-3882-42bb-d1cf-638c71556d94"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "tf.executing_eagerly()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoGfulMAXIA6",
        "outputId": "6e39e67b-e39a-4f1d-b7c4-9be77aaf01d0"
      },
      "source": [
        "# Confirms that preprocessing.StringLookup is an ABC and not an instance\n",
        "type(preprocessing.StringLookup), isinstance(preprocessing.StringLookup, preprocessing.StringLookup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(abc.ABCMeta, False)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cjZsxWmXIEA"
      },
      "source": [
        "We calculated paras at the start of this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AICMKuPedv7I"
      },
      "source": [
        "parase_set = set(paras)\n",
        "vocab = sorted(set(paras))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgVtjOk3XIHh"
      },
      "source": [
        "chars = tf.strings.unicode_split(list(vocab), input_encoding='UTF-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyERPkEWY_KY",
        "outputId": "67f2c9c3-0306-44df-ffe3-8a1f9a1c4257"
      },
      "source": [
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b' '], [b'!'], [b'\"'], [b'$'], [b'&'], [b\"'\"], [b'('], [b')'], [b','], [b'-'], [b'.'], [b'/'], [b'0'], [b'1'], [b'2'], [b'3'], [b'4'], [b'5'], [b'6'], [b'7'], [b'8'], [b'9'], [b':'], [b';'], [b'?'], [b'A'], [b'B'], [b'C'], [b'D'], [b'E'], [b'F'], [b'G'], [b'H'], [b'I'], [b'J'], [b'K'], [b'L'], [b'M'], [b'N'], [b'O'], [b'P'], [b'Q'], [b'R'], [b'S'], [b'T'], [b'U'], [b'V'], [b'W'], [b'X'], [b'Y'], [b'Z'], [b'['], [b']'], [b'a'], [b'b'], [b'c'], [b'd'], [b'e'], [b'f'], [b'g'], [b'h'], [b'i'], [b'j'], [b'k'], [b'l'], [b'm'], [b'n'], [b'o'], [b'p'], [b'q'], [b'r'], [b's'], [b't'], [b'u'], [b'v'], [b'w'], [b'x'], [b'y'], [b'z'], [b'\\xc3\\xa7'], [b'\\xc3\\xa8'], [b'\\xc3\\xa9'], [b'\\xc3\\xaa'], [b'\\xc3\\xb4'], [b'\\xe2\\x80\\x93'], [b'\\xe2\\x80\\x94']]>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbziKQDudCOo"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lYG0sazeKVw",
        "outputId": "ac86bdf1-141d-4a6f-9b50-78950aba05c3"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41], [42], [43], [44], [45], [46], [47], [48], [49], [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], [62], [63], [64], [65], [66], [67], [68], [69], [70], [71], [72], [73], [74], [75], [76], [77], [78], [79], [80], [81], [82], [83], [84], [85], [86]]>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17EX6Yxf1gVl"
      },
      "source": [
        "\n",
        "##8.Build a Recurrent Neural Network (RNN) model using keras with the following parameters:\n",
        "##-vocab_size = len(vocab)\n",
        "##-embedding_dim = 256 #\n",
        "##-rnn_units = 1024\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvdidVc1G_vz",
        "outputId": "4e22c999-840f-44b5-8e84-c5893c2d2a76"
      },
      "source": [
        "# creating training examples and targets\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(paras,'UTF-8'))\n",
        "type(all_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZziX08hdG_zC",
        "outputId": "2b005d11-a96c-4852-af57-9d1560f8101c"
      },
      "source": [
        "all_ids.get_shape()\n",
        "# This creates a dataset whose elements are slices from the original tensor\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "type(ids_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.TensorSliceDataset"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP0qRd7QKCNJ"
      },
      "source": [
        "# commenting it to avoid the long output\n",
        "#temp = ids_dataset.unique()\n",
        "#for element in temp:\n",
        "#  print(element.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFWbOUE0g4ZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6eb7890-abd8-4344-fe93-f5232ac96973"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkAEKs04I7dQ"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim =256\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H7DgaQghTS6"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "\n",
        "    # What's this layer?\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    # What's this layer?\n",
        "    self.rnn = tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    \n",
        "    # What's this layer?\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.rnn.get_initial_state(x)\n",
        "    x, states = self.rnn(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuWyu2JjrzA"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kK69OhykAHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dc2a4a-e967-4d14-d4ce-003076c12d3a"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.MyModel"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOa-Ko6j1iRD"
      },
      "source": [
        "\n",
        "##9.Train your RNN for 20 EPOCHS.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJdCIP7PI8ZC"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDvkC1NDjUnG"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVmn8dbOkXNP"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH1kpAKDk5Tg"
      },
      "source": [
        "seq_length = 80 # About one line of standard text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G45Vb_rRk0Fn"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTUAxxeWkSXm"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY3WGlVvjUq6"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset:\n",
        "# TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements.\n",
        "EPOCHS = 20\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y1QMXtmj9nx",
        "outputId": "0a2817f3-5c07-439e-df3e-e6f3b8a8ee16"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "18/18 [==============================] - 23s 1s/step - loss: 3.5391\n",
            "Epoch 2/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 3.1140\n",
            "Epoch 3/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.9246\n",
            "Epoch 4/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.6933\n",
            "Epoch 5/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.5501\n",
            "Epoch 6/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.4342\n",
            "Epoch 7/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.3568\n",
            "Epoch 8/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.2897\n",
            "Epoch 9/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.2277\n",
            "Epoch 10/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.1755\n",
            "Epoch 11/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.1245\n",
            "Epoch 12/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.0745\n",
            "Epoch 13/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 2.0305\n",
            "Epoch 14/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 1.9849\n",
            "Epoch 15/20\n",
            "18/18 [==============================] - 20s 1s/step - loss: 1.9357\n",
            "Epoch 16/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 1.8979\n",
            "Epoch 17/20\n",
            "18/18 [==============================] - 20s 1s/step - loss: 1.8491\n",
            "Epoch 18/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 1.8074\n",
            "Epoch 19/20\n",
            "18/18 [==============================] - 21s 1s/step - loss: 1.7718\n",
            "Epoch 20/20\n",
            "18/18 [==============================] - 20s 1s/step - loss: 1.7245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJjAKSRbjUua"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtettX021j6W"
      },
      "source": [
        "\n",
        "##10. Generate a new sentence based on the original Wikipedia article you chose and seed the model with the first three words in the first sentence of the article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA91hkFv1lDn"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uECmu5btkpu"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux6PUmrVtZQX"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ALNXnvfuo35"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmii4JDptcCs",
        "outputId": "15035c49-ac8b-414d-98c8-578eba73b369"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Thomas_Jefferson was'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thomas_Jefferson was ace mase plences, arch duilary. On the S.S. Jatus an 1801. Oe duthy ardemste castral towend Univelsatevingluw tam Store. Encompariagly, and there Sawer conturlan. The resurnctory and proves Marmyen Castilite in manaye tox atonn recorn and and erpontand tot Burtis gromolit ond abaic as to 1706, Jefferson a the State of Statealis. Montices, relowes cerianirg binlica, \"piyta Pancied the Freendent of his now tre teed fabed \". 170k wall and he toll fiden beri, in 1906, Jefferson was cralia espaise-viling, \"th-sepaloniry effreds in tied the Renellion for in Nather by whing utley He itsy and Franct merkning on Ix 1774, Burs cellanc into-ualoro-somitiag ith ol the alinay to rigltanth o pralis, a s arcessming Corminicy on 1104, Share to pround, and of camte soputain sationed Wiitas. Slarch (derawas and parsho and ftribling but pliburqy in the secay and vilieve be his astorized bottry enminding wimh EnS6 polis lew Mantuan, Jefferson bat noth Matist appisciften tho Decial in expershatit koy was  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 1.4581122398376465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSJxSCP2uxdU",
        "outputId": "3bd88821-054b-4133-b33c-0a4655f9edf0"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Thomas_Jefferson was', ' Founding Father who', 'As president, Jefferson'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Thomas_Jefferson was. Averivach inT180 Eed cassitices, ente more\\xe2\\x80\\x93\"tion lisiated Tongress bnizess, ther an thror. .0. fele and the iale is destrouncembear\\'s in 1803 and orsa pupligation was a labe Br the .0. Aht of his enfiter, he aftecredonitall Manrot appostof copisillutitizes proent brock mideened thay, After Adexulleatith grection\\'s Congrce by ade doan of the Lacy of lencead, th cener Paned, the Twote his dould Or vilatory Comme, Helinial inder Marizanized at id Lacued repyed the Mings of the UX2. The agco, his menber nears, in dill bsong leass and was a preainal Conmmargm. His liftite of Vircanian, at Jefferson. In 1648 eeslasad Colitien.  Ithe \"AcCpadine Prssinial ammate\" Selitsoul in Jeltoul S.S presitert songery fouts lat ase Ecsoush mad y arg of tie cour Peseraced as hingltures as dait nevern tiatlly. Wil the inent an  on N0-Ne Virginia one to rolomien act was elte ens a utialutict, EnCinetteas hawern[stronand thaigt of 1754) his oble namonty, an aintn with rerigns of in Wasquake Inwrye an affurmi'\n",
            " b' Founding Father who in 17n4) Denigul has antuelly byea Preedeng. Vion purion An oTris heuguty, aphies  exulaim, rohin thricas ry was Aothrreaty our Publing  Jeszen of his ustop lelagenty ald asmande uacianad sor blatied wosh theed,\" Duming un Acripanaly writ thries by the Duveral hounc wly pigsthed 1761; by wroking lere ar Marisnal would in Grance to and statying to dichilad ingh in the 1804 to Vithinis profine frions aivionoly Hestation\\'s upplion, SMadushed a bedistsen and his iniz rtath his clion\\' of thetien thre, aplavar hivernd toke to as inatiot woth w of Evancart and das and wish pally wariss fourdesse lister WiN-Jene-Rerntulingr trar, Revernment affice.  are visworch id h indunes, and desure foun but Jeffermon offrains of Stion lesor as inglunine. In the laver by the Lanisinisis, botis leve rece fres utter innis recbment, Aferion, it has lowar and the dmbecils in Juby his presond trate war unfited Presind it conariby Parlipton, his ofe canslont ligracy  ard his noubst cinclad his realing rient fol'\n",
            " b'As president, Jefferson while $9 to Willination aspupted silary Suavariet for minitabl on phol bol tis ecterne thickson repzesed acauss on F song tow stater bick enames and detinaliver wash Manteaded terrifines wor and Willican Collimen (Ats a payion\\'s pundy and villuctions neritanien of sto munting to Seiforear hownal to nomully inin wrrcise pedertorist intraind thipkirg Frentwen the willing mien of his oun and Mont-ormistos \". Frenuning and seact\", bublar att oching in \"mo ta Male of anderacilte ived as propesen. Stmy assss chitle Pansing democen lisslly in 1774, Buring hid ign a mound it cammer and for for Spang Finitars of Gordy an seeven encoloned s,erthin. The \\xc3\\xb4an siffrements al 5 camon Purina His wive politit, the visl enecon on belibar a clivige mort of state lewides ional dibinm the \"allarer 1706. Jefferson was gould butinion, to Jale ar a baril ry state trates and \"unmetins. Sen & be dena; Jefferson fod to plestor. In the kes risht expallecco if 1792\\xe2\\x80\\x93), in 1796, and as e creauded vime lavist Priton'], shape=(3,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 1.631469964981079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYURhlkquJ7L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}