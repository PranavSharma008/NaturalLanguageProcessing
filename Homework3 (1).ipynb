{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NEQgYTllZ0P"
      },
      "source": [
        "# 1. Write a function that uses beautifulsoup4 and lxml to scrape each Wikipedia page. Organize the function so it returns a list of paragraphs, where each paragraph is a text string. If you need help with scraping Wikipedia, refer back to your previous homework assignment.\n",
        "\n",
        "# 2. Every Wikipedia article begins with a very general introductory paragraph that is like an abstract for the rest of the article. Extract just the introductory paragraph text from your list of paragraphs for each article. The way the bs/lxml parsing works, you will find that the introductory paragraph is usually at index 1 (i.e., it is the second paragraph in the list). You should end up with three introductory paragraphs.\n",
        "\n",
        "\n",
        "# 3. For this task and the remaining tasks below, it is fine to write three separate steps (to cover each of the three paragraphs), but feel free to process the data in a loop, if you want to make your code more elegant. Run a Wikipedia text cleaner on each paragraph to remove square brackets footnotes and extra spaces. If you need help with this, refer back to your previous homework assignment.\n",
        "\n",
        "\n",
        "# 4. Load the pre-trained spaCy pipeline called “en_core_web_lg” and then import and load it. If you need a reminder on how to do this, refer back to Lab 5,    Note that the pipeline takes two or three minutes to download because of the large amount of word embedding data.\n",
        "\n",
        "# 5. Parse each of the introductory paragraphs into separate documents with the spaCy pipeline.\n",
        "\n",
        "\n",
        "# 6. Use the is_stop attribute to filter all of the stop words out of the tokens stream for each introductory paragraph. Also filter out all of the punctuation with is_punct.\n",
        "\n",
        "# 7. As a diagnostic, show the word vector for the first token in the first introductory paragraph. It should be a vector of 300 floating point values.\n",
        "\n",
        "# 8. Compute the mean word vector across all the word vectors in each paragraph. Note that one of the easiest and probably the most efficient way to do this is to put all of the word vectors for each paragraph into its own numpy array. Your result for each paragraph should be a onerow by 300-column vector representing each introductory paragraph.\n",
        "\n",
        "\n",
        "\n",
        "# 9. Use cosine similarity to create similarity scores between each pair of mean word vectors. You should end up with three floating point values bounded between 0 and 1. If you need an example of how to compute cosine similarity, look back at Homework 5.\n",
        "\n",
        "# 10. Interpret the results from the previous step. Which two introductory paragraphs are most similar to each other? Which two are least similar? Does the pattern of values make sense? Why or why not? Write a sentence describing how you could use this summarization technique on a larger scale.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxc-geEqrOAb",
        "outputId": "0d10e244-b43a-4afd-846e-de2170a86aa4"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.6.1.tar.gz (170 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 25.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 61 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 92 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 122 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 153 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 13.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169314 sha256=f447dd4fd4d5e869232e365896ece018ce84a700d684f92376c6403a843d18eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/5f/d3/03d313ddb3c2a1a427bb4690f1621eea60fe6f2a30cc95940f\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMZMp6xHrpUZ",
        "outputId": "d19f324c-3241-4523-cc2d-e0f439835ca4"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install allennlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 5.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n",
            "Collecting allennlp\n",
            "  Downloading allennlp-2.8.0-py3-none-any.whl (738 kB)\n",
            "\u001b[K     |████████████████████████████████| 738 kB 12.2 MB/s \n",
            "\u001b[?25hCollecting cached-path<0.4.0,>=0.3.1\n",
            "  Downloading cached_path-0.3.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.62.3)\n",
            "Requirement already satisfied: torch<1.11.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.9.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.10.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.3.4)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "\u001b[K     |████████████████████████████████| 259 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.2)\n",
            "Collecting sqlitedict\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: torchvision<0.12.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.10.0+cu111)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.23.0)\n",
            "Collecting datasets<2.0,>=1.2.1\n",
            "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 79.7 MB/s \n",
            "\u001b[?25hCollecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting wandb<0.13.0,>=0.10.0\n",
            "  Downloading wandb-0.12.6-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 84.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 57.4 MB/s \n",
            "\u001b[?25hCollecting checklist==0.0.11\n",
            "  Downloading checklist-0.0.11.tar.gz (12.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1 MB 21.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: filelock<3.4,>=3.3 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n",
            "Collecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[K     |████████████████████████████████| 190 kB 77.9 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: transformers<4.13,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.12.3)\n",
            "Collecting munch>=2.5\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n",
            "Collecting patternfork-nosql\n",
            "  Downloading patternfork_nosql-3.6.tar.gz (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting iso-639\n",
            "  Downloading iso-639-0.4.5.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting boto3<2.0,>=1.0\n",
            "  Downloading boto3-1.20.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 66.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<0.4.0,>=0.3.1->allennlp) (1.18.1)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.0\n",
            "  Downloading botocore-1.23.1-py3-none-any.whl (8.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1 MB 51.4 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.0->boto3<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (2.8.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 66.5 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (21.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 83.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (4.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets<2.0,>=1.2.1->allennlp) (1.1.5)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.35.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (57.4.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.26.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (3.17.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (2018.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.16->allennlp) (3.7.4.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.1.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets<2.0,>=1.2.1->allennlp) (2.4.7)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<0.4.0,>=0.3.1->allennlp) (0.4.8)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp) (2021.5.30)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2,>=2.1.0->allennlp) (2.0.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets<2.0,>=1.2.1->allennlp) (3.6.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.12.0,>=0.8.1->allennlp) (7.1.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.13,>=4.1->allennlp) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.13,>=4.1->allennlp) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.13,>=4.1->allennlp) (2019.12.20)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.7-py3-none-any.whl (8.6 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 79.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.1.0-py3-none-any.whl (19 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.12.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 79.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (2.0.7)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 79.4 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets<2.0,>=1.2.1->allennlp) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 55.9 MB/s \n",
            "\u001b[?25hCollecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 46.3 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 46.9 MB/s \n",
            "\u001b[?25hCollecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.0.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.4.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-4.1.2-py3-none-any.whl (15 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.6.0-py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (5.2.2)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.20)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (1.11.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.13,>=4.1->allennlp) (1.0.1)\n",
            "Building wheels for collected packages: checklist, fairscale, overrides, jsonnet, subprocess32, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k, sqlitedict\n",
            "  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165633 sha256=2c40ffe1dbc42288ac5fbefd299a1a3cb26079b62fc95fe34514d41e3cd5abcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=bac932e8b8d3d642c1034aa7aec972acabed36a6b1ea9d84e7a4eda123525ab2\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/8e/a3/7a2f33ac996114b816d88e55cf1235a1e058f30211e39bd719\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=4bbbea3221ae506d8910b76c43f0149fb8dbf9b15be7b18a4ff1b38cf1756b23\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388661 sha256=000984a886d264057b11d0599920a4d0d713a268f5dbdb3b9bb77fbc28c88d04\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=825677d4286e7798634fe94c5a5514565ca352dc53f8b0124bd1d0e0e27d6354\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=cc4425acccc9a5b46dec780d93d6a850048391974d54eaa0d6e98c992b3a6ba3\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=accdc2a4c5ae98b949b633fade75f61025f5fec26c211f53b97130390392ca1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332806 sha256=d6697c2be7c38265583f3850d5af3e396b522ca53e214b95663602e763f0dd74\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=7b1f950b01d06d1689bba4aee5fa98b321d7522bb14b4807450afbad4ab43940\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=7436530b1250beba894cb3fb4ff7cbc5c8721a1ba5d8fcbbf1018e3324f8653f\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=b5af8a9d076458470933cbe1aa361c5a351bfbcfbc5fb219364a4e9ee3064da8\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "Successfully built checklist fairscale overrides jsonnet subprocess32 iso-639 pathtools patternfork-nosql python-docx sgmllib3k sqlitedict\n",
            "Installing collected packages: urllib3, jaraco.functools, tempora, multidict, jmespath, jaraco.text, jaraco.classes, frozenlist, zc.lockfile, yarl, smmap, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, botocore, asynctest, async-timeout, aiosignal, s3transfer, python-docx, pdfminer.six, gitdb, fsspec, feedparser, cherrypy, backports.csv, aiohttp, yaspin, xxhash, subprocess32, shortuuid, sentry-sdk, patternfork-nosql, pathtools, overrides, munch, iso-639, GitPython, docker-pycreds, configparser, boto3, wandb, tensorboardX, sqlitedict, sentencepiece, jsonnet, fairscale, datasets, checklist, cached-path, base58, allennlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.8.0 aiosignal-1.2.0 allennlp-2.8.0 async-timeout-4.0.0 asynctest-0.13.0 backports.csv-1.0.7 base58-2.1.1 boto3-1.20.0 botocore-1.23.1 cached-path-0.3.2 checklist-0.0.11 cheroot-8.5.2 cherrypy-18.6.1 configparser-5.1.0 cryptography-35.0.0 datasets-1.15.1 docker-pycreds-0.4.0 fairscale-0.4.0 feedparser-6.0.8 frozenlist-1.2.0 fsspec-2021.11.0 gitdb-4.0.9 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.4.0 jaraco.text-3.6.0 jmespath-0.10.0 jsonnet-0.17.0 multidict-5.2.0 munch-2.5.0 overrides-3.1.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20211012 portend-3.0.0 python-docx-0.8.11 s3transfer-0.5.0 sentencepiece-0.1.96 sentry-sdk-1.4.3 sgmllib3k-1.0.0 shortuuid-1.0.7 smmap-5.0.0 sqlitedict-1.7.0 subprocess32-3.5.4 tempora-4.1.2 tensorboardX-2.4 urllib3-1.25.11 wandb-0.12.6 xxhash-2.0.2 yarl-1.7.2 yaspin-2.1.0 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5-W3scwll9v"
      },
      "source": [
        "# 1. Write a function that uses beautifulsoup4 and lxml to scrape each Wikipedia page. Organize the function so it returns a list of paragraphs, where each paragraph is a text string. If you need help with scraping Wikipedia, refer back to your previous homework assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rantc0DkuJQ3"
      },
      "source": [
        "url_list = ['https://en.wikipedia.org/wiki/Sachin_Tendulkar','https://en.wikipedia.org/wiki/Steve_Waugh','https://en.wikipedia.org/wiki/Maia_Chiburdanidze']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkiiBb_swM9t",
        "outputId": "7a0cd28d-8e8c-4bbe-d663-77e0b256678a"
      },
      "source": [
        "import nltk\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeeNT84OzBNq"
      },
      "source": [
        "def scrape_wiki(url):\n",
        "  scraped_data = urllib.request.urlopen(url)\n",
        "  article = scraped_data.read()\n",
        "  parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "  paragraphs = parsed_article.find_all('p')\n",
        "  return paragraphs\n",
        "\n",
        "#scraped_data = urllib.request.urlopen(url_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLFHxAevzdCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100f9561-c1cb-4a0f-cf87-4b5e7387543c"
      },
      "source": [
        "para_ur1= scrape_wiki(url_list[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-951e02563d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpara_ur1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mscrape_wiki\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'scrape_wiki' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK3QYRpmzw28"
      },
      "source": [
        "pattern = re.compile('\\[[0-9]*\\]')\n",
        "pattern2 = re.compile('\\s+')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRiK3zps27Lk"
      },
      "source": [
        "# 2. Every Wikipedia article begins with a very general introductory paragraph that is like an abstract for the rest of the article. Extract just the introductory paragraph text from your list of paragraphs for each article. The way the bs/lxml parsing works, you will find that the introductory paragraph is usually at index 1 (i.e., it is the second paragraph in the list). You should end up with three introductory paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zXjrw8B271C"
      },
      "source": [
        "into_url1_paragraph = para_ur1[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmnEW3nq3NF_",
        "outputId": "73dbbfcb-d266-4f71-8732-f4f14ef4a3c9"
      },
      "source": [
        "para_url2=scrape_wiki(url_list[1])\n",
        "intro_url2_paragraph = para_url2[1]\n",
        "intro_url2_paragraph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<p><b>Stephen Rodger Waugh</b> <span class=\"noexcerpt nowraplinks\" style=\"font-size:85%;\"><a class=\"mw-redirect\" href=\"/wiki/Officer_of_the_Order_of_Australia\" title=\"Officer of the Order of Australia\">AO</a></span> (born 2 June 1965) is a former Australian international <a href=\"/wiki/Cricket\" title=\"Cricket\">cricketer</a> and twin brother of cricketer <a href=\"/wiki/Mark_Waugh\" title=\"Mark Waugh\">Mark Waugh</a>. A right-handed batsman, he was also a medium-pace bowler. As Australian captain from 1997 to 2004, he led Australia to fifteen of their record sixteen consecutive Test wins, and to victory in the <a href=\"/wiki/1999_Cricket_World_Cup\" title=\"1999 Cricket World Cup\">1999 Cricket World Cup</a>. Waugh is considered the most successful Test captain in history with 41 victories and a winning ratio of 72%.<sup class=\"reference\" id=\"cite_ref-1\"><a href=\"#cite_note-1\">[1]</a></sup>\n",
              "</p>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF3835kq3Uid",
        "outputId": "cfad7796-464f-42f4-8068-97dcb941aa77"
      },
      "source": [
        "para_url3 = scrape_wiki(url_list[2])\n",
        "intro_url3_paragraph = para_url3[1]\n",
        "intro_url3_paragraph"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<p><b>Maia Chiburdanidze</b> (<a href=\"/wiki/Georgian_language\" title=\"Georgian language\">Georgian</a>: <span lang=\"ka\" title=\"Georgian-language text\">მაია ჩიბურდანიძე</span>; born 17 January 1961) is a <a href=\"/wiki/Georgia_(country)\" title=\"Georgia (country)\">Georgian</a> <a href=\"/wiki/Chess\" title=\"Chess\">chess</a> <a href=\"/wiki/Grandmaster_(chess)\" title=\"Grandmaster (chess)\">Grandmaster</a>. She is the sixth <a href=\"/wiki/Women%27s_World_Chess_Championship\" title=\"Women's World Chess Championship\">Women's World Chess Champion</a>, a title she held from 1978 to 1991, and was the youngest one until 2010, when this record was broken by <a href=\"/wiki/Hou_Yifan\" title=\"Hou Yifan\">Hou Yifan</a>. Chiburdanidze is the second woman to be awarded the title of Grandmaster by <a href=\"/wiki/FIDE\" title=\"FIDE\">FIDE</a>, which occurred in 1984. She has played on nine gold-medal-winning teams in the <a href=\"/wiki/Women%27s_Chess_Olympiad\" title=\"Women's Chess Olympiad\">Women's Chess Olympiad</a>.<sup class=\"reference\" id=\"cite_ref-1\"><a href=\"#cite_note-1\">[1]</a></sup>\n",
              "</p>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nixSITpb3hHO"
      },
      "source": [
        "# 3. For this task and the remaining tasks below, it is fine to write three separate steps (to cover each of the three paragraphs), but feel free to process the data in a loop, if you want to make your code more elegant. Run a Wikipedia text cleaner on each paragraph to remove square brackets footnotes and extra spaces. If you need help with this, refer back to your previous homework assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqyyOamE3hqm"
      },
      "source": [
        "import re\n",
        "pattern = re.compile('\\[[0-9]*\\]')\n",
        "pattern2 = re.compile('\\s+')\n",
        "intro1_text = ''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "v8_2MI8LAGk3",
        "outputId": "c3f4db1e-14aa-45c9-ebf8-b3e5f6272a6d"
      },
      "source": [
        "def wiki_cleaner( text):\n",
        "  listy = pattern.split(text)\n",
        "  text=''\n",
        "  for j in listy:\n",
        "    text += j\n",
        "  listy = pattern2.split(text)\n",
        "  text=''\n",
        "  for j in listy:\n",
        "    text += j + ' '\n",
        "  return text\n",
        "intro1_text = wiki_cleaner(para_ur1[1].text) \n",
        "intro1_text\n",
        "#intro1_list=pattern2.split(intro1_text)\n",
        "#intro1_text=''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Sachin Ramesh Tendulkar (/ˌsʌtʃɪn tɛnˈduːlkər/ (listen); pronounced [sət͡ʃin t̪eːɳɖulkəɾ]; born 24 April 1973) is a former international cricketer of India who served as captain of the Indian national team. He is widely regarded as one of the greatest batsmen in the history of cricket. He is the highest run scorer of all time in international cricket, and the only player to have scored one hundred international centuries, the first batsman to score a double century in a One Day International (ODI), the holder of the record for the most runs in both Test and ODI cricket, and the only player to complete more than 30,000 runs in international cricket. In 2013, he was the only Indian cricketer included in an all-time Test World XI named to mark the 150th anniversary of Wisden Cricketers\\' Almanack. He is affectionately known as \"Little Master\" or \"Master Blaster\".  '"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "SU3ZL27ADepx",
        "outputId": "c2b3a305-a8cf-4034-8286-6933ea5f327b"
      },
      "source": [
        "intro2_text = wiki_cleaner(para_url2[1].text)\n",
        "intro2_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Stephen Rodger Waugh AO (born 2 June 1965) is a former Australian international cricketer and twin brother of cricketer Mark Waugh. A right-handed batsman, he was also a medium-pace bowler. As Australian captain from 1997 to 2004, he led Australia to fifteen of their record sixteen consecutive Test wins, and to victory in the 1999 Cricket World Cup. Waugh is considered the most successful Test captain in history with 41 victories and a winning ratio of 72%.  '"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "fksL2XAADoZE",
        "outputId": "138fcd1d-dd36-41ec-8989-e1a60700c473"
      },
      "source": [
        "intro3_text = wiki_cleaner(para_url3[1].text)\n",
        "intro3_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Maia Chiburdanidze (Georgian: მაია ჩიბურდანიძე; born 17 January 1961) is a Georgian chess Grandmaster. She is the sixth Women's World Chess Champion, a title she held from 1978 to 1991, and was the youngest one until 2010, when this record was broken by Hou Yifan. Chiburdanidze is the second woman to be awarded the title of Grandmaster by FIDE, which occurred in 1984. She has played on nine gold-medal-winning teams in the Women's Chess Olympiad.  \""
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3TaafAQ6q4V"
      },
      "source": [
        "# 4. Load the pre-trained spaCy pipeline called “en_core_web_lg” and then import and load it. If you need a reminder on how to do this, refer back to Lab 5,    Note that the pipeline takes two or three minutes to download because of the large amount of word embedding data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ymUdQxT6rd0",
        "outputId": "32a047f0-32f4-417f-c32b-eb4492a66bf1"
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import spacy.cli # Use the command line interface\n",
        "spacy.cli.download(\"en_core_web_lg\") # This imports the large model onto your virtual machines\n",
        "import en_core_web_lg # Now that it is downloaded, we can import it\n",
        "nlp_lg = en_core_web_lg.load() # Create an instance for further use"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
            "don't seem to have the spaCy package itself installed (maybe because you've\n",
            "built from source?), so installing the model dependencies would cause spaCy to\n",
            "be downloaded, which probably isn't what you want. If the model package has\n",
            "other dependencies, you'll have to install them manually.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKX_sNQa6y3Z"
      },
      "source": [
        "# 5. Parse each of the introductory paragraphs into separate documents with the spaCy pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pIMosCb66eN"
      },
      "source": [
        "docs1 = nlp(intro1_text)\n",
        "docs2 = nlp(intro2_text)\n",
        "docs3= nlp(intro3_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LokSF5Uz7AUR"
      },
      "source": [
        "# 6. Use the is_stop attribute to filter all of the stop words out of the tokens stream for each introductory paragraph. Also filter out all of the punctuation with is_punct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx-zTfdO7A7L"
      },
      "source": [
        "def remove_stop_punct(document):\n",
        "  text_stop = [token for token in document if not token.is_stop]\n",
        "  text_stop_punct = [token for token in document if not token.is_punct]\n",
        "  return text_stop_punct\n",
        "\n",
        "docs1_filtered = remove_stop_punct(docs1)\n",
        "docs2_filtered = remove_stop_punct(docs2)\n",
        "docs3_filtered = remove_stop_punct(docs3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS88zT2q7H-R"
      },
      "source": [
        "# 7. As a diagnostic, show the word vector for the first token in the first introductory paragraph. It should be a vector of 300 floating point values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVrTcVzR7IiD",
        "outputId": "5ea8563a-4097-4794-b076-68ffb30166b7"
      },
      "source": [
        "import numpy as np\n",
        "# printing the first word\n",
        "print(docs1_filtered[0])\n",
        "word_= docs1_filtered[1]\n",
        "Sachin=nlp_lg.vocab[\"word_\"]\n",
        "len(Sachin.vector)\n",
        "Sachin.vector.dtype\n",
        "print(len(Sachin.vector), Sachin.vector.dtype)\n",
        "print ( len(np.array(Sachin.vector)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sachin\n",
            "300 float32\n",
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZV56ZFg7iJg"
      },
      "source": [
        "# 8. Compute the mean word vector across all the word vectors in each paragraph. Note that one of the easiest and probably the most efficient way to do this is to put all of the word vectors for each paragraph into its own numpy array. Your result for each paragraph should be a onerow by 300-column vector representing each introductory paragraph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGrMLowB7iu1",
        "outputId": "3e6dc574-e092-45c1-eadf-f1b3a0cbd048"
      },
      "source": [
        "# an example of a word vector being stored in the numpy array\n",
        "import numpy as np\n",
        "np_Sachin=np.array(Sachin.vector)\n",
        "#np.mean(np_Sachin)\n",
        "np.mean(Sachin.vector)\n",
        "len(Sachin.vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bb4L7ENMKVy"
      },
      "source": [
        "from numpy import asarray, zeros\n",
        "np_docs1=[]\n",
        "i=0\n",
        "for x in docs1_filtered:\n",
        "  xyz =nlp_lg.vocab[\"x\"]\n",
        "  np_docs1.append(xyz.vector)\n",
        "\n",
        "meanVector1=np.mean(np_docs1, axis=0)\n",
        "# mean word vector of introductory paragraph 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qumfdIpFNuiz",
        "outputId": "dc693def-81d4-489e-98b9-022f1eadcd31"
      },
      "source": [
        "np_docs2=[]\n",
        "for x in docs2_filtered:\n",
        "  xyz2= nlp_lg.vocab[\"x\"]\n",
        "  np_docs2.append(xyz2.vector)\n",
        "meanVector2= np.mean(np_docs2,axis=0)\n",
        "len(meanVector2)\n",
        "# mean word vector of introductory paragraph 2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq8GFoKSN7yq",
        "outputId": "3d322819-0f1f-4f4b-d0bd-47fce5232fdd"
      },
      "source": [
        "np_docs3=[]\n",
        "\n",
        "for x in docs3_filtered:\n",
        "  xyz3=nlp_lg.vocab[\"x\"]\n",
        "  np_docs3.append(xyz3.vector)\n",
        "meanVector3= np.mean(np_docs3,axis=0)\n",
        "len(meanVector3)\n",
        " # mean word vector of introductory paragraph 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgoBS0Yh73xA"
      },
      "source": [
        "# 9. Use cosine similarity to create similarity scores between each pair of mean word vectors. You should end up with three floating point values bounded between 0 and 1. If you need an example of how to compute cosine similarity, look back at Homework 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbB54mvnOS8I"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "def cosine(variableA,variableB):\n",
        "    return np.dot(variableA,variableB) / (np.sqrt(np.dot(variableA,variableA)) * np.sqrt(np.dot(variableB,variableB)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1VpUBRakHyF",
        "outputId": "275c126e-6e0e-47d7-d6cc-377f9c2fbb98"
      },
      "source": [
        "cosine(meanVector1, meanVector2)\n",
        "#meanVector1=np.reshape(meanVector1, (300,1))\n",
        "#meanVector3 = np.reshape(meanVector3, (300,1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms4uh03Xke8m",
        "outputId": "2be0141b-93e3-4376-921e-32d668ade7fb"
      },
      "source": [
        "cosine(meanVector3, meanVector2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXefzJ2BkhNz",
        "outputId": "511775cd-60ee-4ccf-cefe-51a1f6dd384a"
      },
      "source": [
        "cosine(meanVector1, meanVector3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCT3NUXW8DwQ"
      },
      "source": [
        "# 10. Interpret the results from the previous step. Which two introductory paragraphs are most similar to each other? Which two are least similar? Does the pattern of values make sense? Why or why not? Write a sentence describing how you could use this summarization technique on a larger scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6_gJW_l8EcE"
      },
      "source": [
        "The mean Vectors that were calculated of three different intro paragraphs have the same sentiment and are very similar to each other. On a large scale the summarization technique can be used to compare the corpus sentiments across multiple sources"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}